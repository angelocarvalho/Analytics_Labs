{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install textract-trp\n",
    "!pip3 install simplejson\n",
    "!pip install pythena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.display import Image, display\n",
    "from trp import Document\n",
    "from PIL import Image as PImage, ImageDraw\n",
    "import time\n",
    "from IPython.display import IFrame\n",
    "import pandas as pd\n",
    "import random\n",
    "import uuid\n",
    "import numpy as np\n",
    "import simplejson\n",
    "import os\n",
    "import datetime\n",
    "import pythena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/opt/ml/metadata/resource-metadata.json') as fh:\n",
    "    metadata = simplejson.loads(fh.read())\n",
    "accountid = metadata['ResourceArn'].split(':')[4]\n",
    "%set_env accountid={accountid}\n",
    "%set_env bucket_name=lab-{accountid}\n",
    "bucket_name = os.getenv('bucket_name')\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "etl_date = x.strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(etl_date) \n",
    "%set_env etl_date={etl_date}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curent AWS Region. Use this to choose corresponding S3 bucket with sample content\n",
    "\n",
    "mySession = boto3.session.Session()\n",
    "awsRegion = mySession.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket that contains sample documents\n",
    "\n",
    "# We are providing sample documents in this bucket so\n",
    "# you do not have to manually download/upload test documents.\n",
    "\n",
    "s3BucketName = \"aws-workshops-\" + awsRegion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Amazon Textract client\n",
    "textract = boto3.client('textract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forms: Key/Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.aws.amazon.com/textract/latest/dg/API_AnalyzeDocument.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document\n",
    "documentName = \"textract-samples/employmentapp.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(url=s3.generate_presigned_url('get_object', Params={'Bucket': s3BucketName, 'Key': documentName})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Amazon Textract\n",
    "response = textract.analyze_document(\n",
    "    Document={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': documentName\n",
    "        }\n",
    "    },\n",
    "    FeatureTypes=[\"FORMS\"])\n",
    "\n",
    "#print(response)\n",
    "\n",
    "doc = Document(response)\n",
    "applicant_df = pd.DataFrame()\n",
    "application_id = uuid.uuid4().hex\n",
    "applicant_df.insert(0, 'application_id', [application_id], True) \n",
    "\n",
    "for page in doc.pages:\n",
    "    # Print fields\n",
    "    print(\"Fields:\")\n",
    "    column_index = 1\n",
    "    for field in page.form.fields:\n",
    "        #print(\"Key: {}, Value: {}\".format(field.key, field.value))\n",
    "        key = str(field.key).lower().replace(':','').replace(' ','_')\n",
    "        value = str(field.value)\n",
    "        applicant_df.insert(column_index, key, [value], True) \n",
    "        column_index = column_index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicant_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document\n",
    "documentName = \"textract-samples/employmentapp.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(url=s3.generate_presigned_url('get_object', Params={'Bucket': s3BucketName, 'Key': documentName})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Amazon Textract\n",
    "response = textract.analyze_document(\n",
    "    Document={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': documentName\n",
    "        }\n",
    "    },\n",
    "    FeatureTypes=[\"TABLES\"])\n",
    "\n",
    "#print(response)\n",
    "\n",
    "doc = Document(response)\n",
    "employment_history_df = pd.DataFrame()\n",
    "employment_history_df['application_id'] = np.nan\n",
    "qtd_columns = 0\n",
    "\n",
    "for page in doc.pages:\n",
    "     # Print tables\n",
    "    \n",
    "    for table in page.tables:\n",
    "        employment_history_list = table.rows\n",
    "        employment_history_list.pop(0)\n",
    "        header = employment_history_list[0]\n",
    "        \n",
    "        lines = []\n",
    "        columns = []\n",
    "        \n",
    "        for r, row in enumerate(employment_history_list):\n",
    "            line = []\n",
    "            for c, cell in enumerate(row.cells):\n",
    "                #print(\"Table[{}][{}] = {}\".format(r, c, cell.text))\n",
    "                if r == 0:\n",
    "                    qtd_columns = qtd_columns + 1\n",
    "                    column_name = str(cell.text).lower().strip().replace(' ','_')\n",
    "                    employment_history_df[column_name] = np.nan\n",
    "                    columns.append(column_name)\n",
    "                else:\n",
    "                    line.append(str(cell.text.strip()))\n",
    "                    if (len(line) == qtd_columns):\n",
    "                        lines.append(line)\n",
    "\n",
    "employment_history_df = pd.DataFrame(lines, columns=columns)\n",
    "employment_history_df['application_id'] = application_id\n",
    "\n",
    "employment_history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applicant_df.to_parquet(\n",
    "        \"s3://\" + \n",
    "        os.getenv('bucket_name') + \n",
    "        \"/data/analytics/applicant/applicant_\" +\n",
    "        etl_date + \n",
    "        \".parquet.snappy\")\n",
    "\n",
    "employment_history_df.to_parquet(\n",
    "        \"s3://\" + \n",
    "        os.getenv('bucket_name') + \n",
    "        \"/data/analytics/employment_history/employment_history_\" +\n",
    "        etl_date + \n",
    "        \".parquet.snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pythena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('athena', region_name=\"us-east-2\")\n",
    "\n",
    "client.start_query_execution(\n",
    "    QueryString='drop table default.applicant',\n",
    "    ResultConfiguration={'OutputLocation': 's3://' + bucket_name + '/output/'})\n",
    "\n",
    "client.start_query_execution(\n",
    "    QueryString='drop table default.employment_history',\n",
    "    ResultConfiguration={'OutputLocation': 's3://' + bucket_name + '/output/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('create_applicant.sql') as ddl:\n",
    "    client.start_query_execution(\n",
    "        QueryString=ddl.read().format(bucket_name), \n",
    "        ResultConfiguration={'OutputLocation': 's3://' + bucket_name + '/output/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('create_employment_history.sql') as ddl:\n",
    "    client.start_query_execution(\n",
    "        QueryString=ddl.read().format(bucket_name), \n",
    "        ResultConfiguration={'OutputLocation': 's3://' + bucket_name + '/output/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_client = pythena.Athena(database=\"default\", region=\"us-east-2\") \n",
    "\n",
    "sql = \"\"\"\n",
    "select * \n",
    "from applicant a join employment_history e \n",
    "on a.application_id = e.application_id\n",
    "\"\"\"\n",
    "\n",
    "print(sql)\n",
    "\n",
    "df_join, exec_id = athena_client.execute(sql)\n",
    "\n",
    "df_join"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
